## 1.1 计算
### 1.1.1 起泡排序
1. 按照某种约定的次序，将给定的一组元素顺序排列，比如将n个整数按通常的大小次序排成一个非降序列。这类操作统称 #排序 （sorting）。（D.Kruth）
2. #局部有序 与 #整体有序 ：在由一组整数组成的序列$A[0, n-1]$中，满足 $A[i-1]\le A[i]$的相邻元素称作顺序的；否则是逆序的。有序序列中每一对相邻元素都是顺序的，亦即，对任意$1 \le i < n$都有$A[i-1]\le A[i]$；反之，所有相邻元素均顺序的序列，也必然整体有序。
3. 基于有序序列的特征，可以通过不断改善局部的有序性实现整体的有序：从前向后依次检查每一对相邻元素，一旦发现逆序即交换二者的位置。对于长度为n的序列，共需做n-1次比较和不超过n-1次交换，这一过程称作一趟 #扫描交换 。
4. 通过多次的扫描交换，直到序列中不再含有任何逆序的相邻元素，使得越小（大）的元素朝上（下）方移动，直至抵达它们各自应处的位置。排序过程中，所有元素朝各自最终位置亦步亦趋的移动过程，犹如起泡在水中上下沉浮，因此得名 #起泡排序 （bubblesort）算法。 ^557857
```cpp
void bubblesort1A(int A[], int n){
	bool sorted = false;  // 整体排序标志，首先假定尚未排序
	while (!sorted){  // 在尚未确认已全局排序之前，逐趟进行扫描交换
		sorted = true;  // 假定已经排序
		for (int i=1;i<n;i++){  // 自左向右逐对检查当前范围A[0,n)内的各相邻元素
			if (A[i-1] > A[i]){  // 一旦A[i-1]与A[i]逆序，则
				swap (A[i-1],A[i]);  // 交换之
				sorted = false;  // 因整体排序不能保证，需要清除排序标志
			}
		}
		n--;  // 至此末元素必然就位，故可以缩短待排序序列的有效长度
	}
}  // 借助布尔型标志位sorted，可及时提前退出，而不致总是蛮力地做n-1趟扫描交换
```

^f50b32

### 1.1.2 算法
1. 计算模型 = 计算机 = 信息处理工具
2. 所谓 #算法 ，即特定计算模型下，旨在解决特定问题的指令序列。算法组成要素：
   - #输入 待处理的信息，对所求解问题特定实例的这种描述称作输入（input）。
   - #输出 经计算和处理的信息，即针对输入问题实例的答案，称作输出（output）。在物理上，输出有可能存放于单独的存储空间中，也可能直接存放于原输入所占的存储空间中。
   - #确定性 算法应可描述为若干语义明确的基本操作组成的指令序列。
   - #可行性 每一基本操作在对应的计算模型中均可实现，且在常数时间内完成。
   - #有穷性 （finiteness） 任意算法都应在执行有限次基本操作之后终止并给出输出。
   - #正确性 （correctness）所给的输出应能够符合由问题本身在事先确定的条件。
3. 证明算法有穷性和正确性的一个重要技巧，就是从适当的角度审视整个计算过程，并找出其所具有的某种不变性和单调性。其中 #单调性 是指，问题的有效规模会随着算法的推进不断递减。 #不变性 则不仅应在算法初始状态下自然满足，而且应与最终的正确性相呼应，当问题的有效规模缩减到0时，不变性应随即等价于正确性。
4. 起泡排序算法的不变性和单调性可分别概括为：经过k趟扫描交换之后，最大的前k个元素必然就位；经过k趟扫描交换之后，待求解问题的有效规模将缩减至n-k。
5. #鲁棒性 （robustness）要求算法尽可能充分地应对各种极端情况。
6. #重用性 算法的总体框架能便捷地推广至其它场合。
7. 好算法
   1. 正确
      - 符合语法，能够编译、链接
      - 能够正确处理简单的输入
      - 能够正确处理大规模的输入
      - 能够正确处理一般性的输入
      - 能够正确处理退化的输入
      - 能够正确处理任意合法的输入
   2. 健壮：能够辨别不合法的输入并做适当处理，而不致非正常退出
   3. 可读：结构化 + 准确命名 + 注释
   4. 效率：速度尽可能快，存储空间尽可能少
      - Algorithms + Data Structures = Programs
      - （Algorithms + Data Structures）* Efficiency = Computation
## 1.2 复杂度度量
### 1.2.1 时间复杂度
问题实例的规模往往是决定计算成本的主要因素。一般地，问题规模越接近，相应的计算成本也越接近；而随着问题规模的扩大，计算成本通常也呈上升趋势。如此，执行时间这一变化趋势可表示为输入规模的一个函数，称作该算法的 #时间复杂度 （time complexity）。具体地，**特定算法处理规模为n的问题所需的时间**可记作T(n)。从保守估计的角度出发，在规模为n的所有输入中选择执行时间最长者作为T(n)，并以T(n)度量该算法的时间复杂度。
### 1.2.2 渐进复杂度
1. 由于同一算法、同一输入，在不同的硬件平台上、不同的操作系统中，甚至不同的时间，所需要的计算时间都不尽相同，因此，不妨将T(n)定义为**算法所执行基本操作的总次数**。也就是说，T(n)决定于组成算法的所有语句各自的执行次数，以及其中所含基本操作的数目。若将算法处理长度为n的序列所需的时间记作T(n)，则只需统计出该算法所执行基本操作的总次数，即可确定T(n)的上界。
2. #渐进分析 （Asymptotic analysis）：当 n >> 2后，对于规模为n的输入，算法需执行的基本操作次数T(n)的值，与需占用的存储单元数S(n)的值都通常可不考虑。这种**着眼长远、更为注重时间复杂度的总体变化趋势和增长速度的策略和方法**，即所谓的渐进分析。
3. #大O记号 （big-O notation）：算法悲观的情况，T(n)的 #渐进上界 。
	- 具体地，若存在正的常数c和函数f(n)，使得对任何 $n>>2$都有 $T(n)\le c\cdot f(n)$，则可认为在n足够大之后，f(n)给出了T(n)增长速度的一个渐进上界。此时记之为：$T(n)=O(f(n))$。即$当 T(n) = O(f(n)), \exists c > 0, 且 n >> 2后，有 T(n)\le c.f(n)$。与T(n)相比，f(n)更为简洁，但依然反映前者的增长趋势。
	- 由这一定义，可导出大O记号的以下性质：
		- 对于任一常数c>0，有$O(f(n)) = O(c\cdot f(n))$。在大O记号的意义下，函数各项正的常系数可以忽略并等同于1。
		- 对于任意常数a>b>0，有$O(n^a + n^b) = O(n^a)$。多项式中的低次项均可忽略，只需保留最高次项。
	- `bubblesort1A()`算法由内、外两层循环组成。内循环从前向后，依次比较各对相邻元素，如有必要则将其交换。故在每一轮内循环中，需要扫描和比较n-1对元素，至多需要交换n-1对元素。元素的比较和交换，都属于基本操作，故每一轮内循环至多需要执行2(n-1)次基本操作，外循环至多执行n-1轮。因此，总共需要执行的基本操作不会超过 $2(n-1)^2$次。若以此来度量该算法的时间复杂度，则有 $T(n)=O(2(n-1)^2)$，根据大O记号的性质，可进一步简化和整理为：$T(n)=O(2n^2-4n+2)=O(2n^2)=O(n^2)$。
	- 由此可见，以大O记号形式表示的时间复杂度，实质上是对算法执行时间的一种保守估计——对于规模为n的任意输入，算法的运行时间都不会超过O(f(n))。比如，起泡排序算法复杂度 $T(n)=O(n^2)$意味着，该算法处理任何序列所需的时间绝不会超过 $O(n^2)$。的确需要这么长计算时间的输入实例，称作 #最坏实例 或 #最坏情况 （worst case）。
	- 保守估计并不排斥更好情况甚至最好情况（best case）的存在和出现。有时也需要考查所谓的平均情况（average case），也就是按照某种约定的概率分布，将规模为n的所有输入对应的计算时间加权平均。但最坏情况复杂度是人们最为关注且使用最多的，在一些特殊场合甚至是唯一的指标。如控制核电站运转、管理神经外科手术室现场的系统，从最好或平均角度评判算法的响应速度都不具有任何意义，在最坏情况下的响应速度才是唯一的指标。
4. 大$\Omega$记号（big-Omega notation）：估计算法复杂度最好的情况
	- 如果存在正的常数c和函数g(n)，使得对于任何$n>>2$都有$T(n)\ge c\cdot g(n)$，就可以认为，在n足够大之后，g(n)给出了T(n)的一个 #渐进下界 。记之为：$T(n)=\Omega(g(n))$。即$T(n) = \Omega(g(n)): \exists c>0\land n >> 2\to T(n)\ge c\cdot g(n)$。与大O记号相反，大$\Omega$记号是对算法执行效率的乐观估计——对于规模为n的任意输入，算法的运行时间都不低于$\Omega(g(n))$。比如，即便在最好的情况下，起泡排序也至少需要$T(n)=\Omega(n)$的计算时间。
5. 大$\Theta$记号（big-Theta notation）：big-O和big-Omega的组合，对算法复杂度的准确估计
	- 借助大O记号、大$Omega$记号，可以对算法的时间复杂度做出定量的界定，即从渐进的趋势看T(n)介于$\Omega(g(n))与O(f(n))$之间。若恰巧出现g(n)=f(n)的情况，则可以用大$\Theta$记号来表示。若存在正的常数$c_1<c_2$和函数h(n)，使得对于任何n>>2都有$c_1\cdot h(n)\le T(n)\le c_2\cdot h(n)$，就可以认为在n足够大之后，h(n)给出了T(n)的一个 #确界 。记之为：$T(n)=\Theta(h(n))$。即 $T(n) = \Theta (f(n)): \exists c_1 > c_2 > 0\land n >> 2\to c_1 \cdot f(n) > T(n) > c_2 \cdot f(n)$。它是对算法复杂度的准确估计——对于规模为n的任何输入，算法的运行时间T(n)都与$\Theta(h(n))$同阶。![[1_渐进分析的记号.png]]
### 1.2.3 空间复杂度
除了执行时间的长短，**算法所需存储空间的多少**也是衡量其性能的一个重要方面，称为 #空间复杂度 （space complexity）。以上针对时间复杂度所引入的几种渐进记号，也适用于对空间复杂度的度量，其原理和方法基本相同。除非特别申明，空间复杂度通常并不计入原始输入本身所占用的空间，其它（如转储、中转、索引、映射、缓冲等）各个方面所消耗的空间，则都应计入。
由于就渐进复杂度的意义而言，在任一算法的任何一次运行过程中所消耗的存储空间，都不会多于其间所执行基本操作的累计次数。因此多数时候**只需关注算法的时间复杂度，而无需对空间复杂度做专门考查**。根据定义，每次基本操作所涉及的存储空间，都不会超过常数规模；纵然每次基本操作所占用或访问的存储空间都是新开辟的，整个算法所需的空间总量，也不过与基本操作的次数同阶。因此，**时间复杂度本身就是空间复杂度的一个天然的上界**。
## 1.3 复杂度分析
大O记号将各算法的复杂度由低到高划分为若干层次级别。
### 1.3.1 常数$O(1)$
$$\begin{align}
&ordinaryElement(S[], n) //从n\ge 3个互异整数中，除最大、最小者以外，任取一个常规元素 \\
&  任取三个元素x, y, z\in S; //这三个元素必互异 \\
&  通过比较，对它们做排序; //设经排序后，依次重命名为：a<b<c\\
&  输出b;
\end{align}$$
$S[]$是有限集，其中的最大、最小元素各有且仅有一个。因此，无论S的规模有多大，在任意三个元素中都至少有一个是非极端元素。取前三个元素x=S[0],y=S[1],z=S[2]，这一步只需执行三次（从特定单元读取元素的）基本操作，耗费O(3)时间。为确定这三个元素的大小次序，最多需要做三次比较，也需O(3)时间。最后，输出居中的非极端元素只需O(1)时间。因此，`ordinaryElement()`算法的运行时间为：$T(n)=O(3)+O(3)+O(1)=O(7)=O(1)$。
运行时间可表示和度量为$T(n)=O(1)$的这一类算法，统称为 #常数时间复杂度算法 （constant-time algorithm）。此类算法已是最为理想的了，因此不可能奢望不劳而获。**一般地，仅含一次或常数次基本操作的算法均属此类，此类算法通常不含循环、分支、子程序调用等，但也不能仅凭语法结构的表面形式一概而论。**
除了输入数组等参数之外，该算法仅需常熟规模的辅助空间。此类仅需O(1)辅助空间的算法，亦称作 #就地算法 （in-place algorithm）。
### 1.3.2 对数$O(\log n)$
```cpp
// 对任意非负整数，统计其二进制展开中位数1的总数
int countOnes(unsigned int n){  // 统计整数二进制展开中数位1的总数：O(log n)
	int ones = 0;  // 计数器复位
	while (0 < n){  // 在n缩减至0之前，反复地检查最低位
		ones += (1 & n);  // 若为1则计数
		n >>= 1;  // 并右移一位
	}
	return ones;  // 返回计数
}  // 等效于glibc的内置函数int __builtin_popcount(unsigned int n)
```
根据右移运算的性质，每右移一位，n都至少缩减一半。也就是说，至多经过 $1+\lfloor\log_2n\rfloor$次循环，n必然缩减至0，从而算法终止。而$1+\left\lfloor\log_2n\right\rfloor$恰为n二进制展开的总位数，每次循环都将其右移一位，总的循环次数自然也是$1+\left\lfloor\log_2n\right\rfloor$。无论是该循环体之前、之内还是之后，均只涉及常数次（逻辑判断、位与运算、加法、右移等）基本操作。因此，`countOnes()`算法的执行时间主要由循环的次数决定，即 $O(1+\lfloor \log_2n\rfloor) = O(\lfloor \log_2n\rfloor)=O(\log_2n)$。
由大O记号定义，在用函数$\log_rn$界定渐进复杂度时，常底数r的具体取值无所谓，故通常不予专门标出而笼统地记作$\log n$。比如，尽管此处底数为常数2，却可直接记作$O(\log n)$。此类算法具有 #对数时间复杂度 （logarithmic-time algorithm）。
更一般地，凡运行时间可以表示和度量为$T(n)=O(\log^cn), c>0$形式的这一类算法，均统称为 #对数多项式时间复杂度算法 （polylogarithmic-time algorithm）。$O(\log n)$即c=1的特例。此类算法的效率虽不如常数复杂度算法理想，但从多项式角度看仍能无限接近于后者，故也是极为高效的一类算法。
### 1.3.3 线性$O(n)$
```cpp
// 计算给定n个整数的总和
int sumI(int A[], int n){  // 数组求和算法（迭代版）
	int sum = 0;  // 初始化累计器，O(1)
	for (int i = 0; i < n; i++)  // 对全部共O(n)个元素逐一累计
		sum += A[i];  // O(1)
	return sum;  // 返回累计值，O(1)
}
```
对s的初始化需要O(1)时间。算法的主体部分是一个循环，每一轮循环中只需进行一次累加运算，这属于基本操作，可在O(1)时间内完成。每经过一轮循环，都将一个元素累加至s，故总共需要做n轮循环，于是该算法的运行时间应为：$O(1)+O(1)\times n = O(n+1)=O(n)$。凡运行时间可以表示和度量为$T(n)=O(n)$形式的这一类算法，均统称为 #线性时间复杂度算法 （linear-time algorithm）。对于输入的每一单元，此类算法平均消耗常数时间。就大多数问题而言，在对输入的每一单元均至少访问一次之前，不可能得到解答。以数组求和为例，在尚未得知每一元素的具体数值之前，绝不可能确定其总和。故此类算法的效率亦足以令人满意。
### 1.3.4 多项式$O(polynomial(n))$
若运行时间可以表示和度量为$T(n)=O(f(n))$的形式，而且f(x)为多项式，则对应的算法称作 #多项式时间复杂度算法 （polynomial-time algorithm）。如[[#^557857|bubblesort1A()]]算法的时间复杂度应为$T(n)=O(n^2)$，故该算法即属于此类。线性时间复杂度算法，也属于多项式时间复杂度算法的特例，其中线性多项式f(n)=n的次数为1。
在算法复杂度理论中，多项式时间复杂度被视作一个具有特殊意义的复杂度级别。多项式级的运行时间成本，在实际应用中一般被认为是可接受的或可忍受的。某问题若存在一个复杂度在此范围以内的算法，则称该问题是 #有有效解的 或 #易解的 （tractable）。
### 1.3.5 指数$O(2^n)$
```cpp
// 在禁止超过1位的位移运算的前提下，对任意非负整数n，计算幂2^n
__int64 power2BF_I(int n){  // 幂函数2^n算法（蛮力迭代版），n>=0
	__int64 pow = 1;  // O(1):累积器初始化为2^0
	while (o < n --)  // O(n): 迭代n轮
		pow <<= 1;  // O(1): 每轮都将累积器翻倍
	return pow;  // O(1): 返回累积器
}  // O(n)=O(2^r), r为输入指数n的比特位数
```
`power2BF_I()`由n轮迭代组成，各需做一次累乘和一次递减，均属于基本操作，故整个算法共需O(n)时间。若以输入指数n的二进制位数$r=1 + \lfloor \log_2n\rfloor$作为输入规模，则运行时间为$O(2^r)$。
一般地，凡运行时间可以表示和度量为$T(n)=O(a^n), a>1$形式的算法，均属于 #指数时间复杂度算法 （exponential-time algorithm）。
从常数、对数、线性到多项式时间复杂度，算法效率的差异还在可接受的范围。然而，在多项式与指数时间复杂度之间，却有着一道巨大的鸿沟。当问题规模较大后，指数复杂度算法的实际效率将急剧下降，计算时间之长很快会达到令人难以忍受的地步。因此通常认为，指数复杂度算法无法真正应用于实际问题中，它们不是有效算法，甚至不能称作算法。相应地，不存在多项式复杂度算法的问题，也称作 #难解的 （intractable）问题。
### 1.3.6 复杂度层次
![[2_复杂度层次.png]]
利用大O记号，不仅可以定量地把握算法复杂度的主要部分，而且可以定性地由低至高将复杂度划分为若干层次。典型的复杂度层次包括$O(1), O(\log^{*}n), O(\log\log n), O(\log n), O(\sqrt(n)), O(n), O(n\log^{*}n), O(n\log\log n), O(n\log n), O(n^2), O(n^3), O(n^c), O(2^n)$等。
### 1.3.7 输入规模
对算法复杂度的界定，都是相对于问题的输入规模而言的。然而，不同的人在不同场合下关于输入规模的理解、定义和度量可能不尽相同，因此导致复杂度分析的结论有所差异。比如`countOnes()`算法的复杂度为$O(\log n)$的结论，是对于输入整数本身的数值n而言的；若以n二进制展开的宽度 $r= 1+\lfloor \log_2n\rfloor$作为输入规模，则应为线性复杂度O(r)。而`power2BF_I()`算法的复杂度为$O(2^r)$的结论，是对于输入指数n的二进制数位r而言的；若以n本身的数值作为输入规模，却应为线性复杂度O(n)。
严格地说，所谓待计算问题的输入规模，应严格定义为**用以描述输入所需的空间规模**。以输入参数n本身的数值作为基准而得出的$O(\log n)和O(n)$复杂度，则应分别称作 #伪对数的 （pseudo-logarithmic）和 #伪线性的 （pseudo-linear）复杂度。
## 1.4 递归
分支转向是算法的灵魂；函数和过程及其之间的相互调用，是在经过抽象和封装之后，实现分支转向的一种重要机制；而 #递归 则是函数和过程调用的一种特殊形式，即**允许函数和过程进行自我调用**。因其高度的抽象性和简洁性，递归已成为多数高级程序语言普遍支持的一项重要特性。比如在C++语言中，递归调用（recursive call）就是某一方法调用自身。这种自我调用通常是直接的，即在函数体中包含一条或多条调用自身的语句。递归也可能以间接的形式出现，即某个方法首先调用其它方法，再辗转通过其它方法的相互调用，最终调用起始的方法自身。递归的价值在于，许多应用问题都可简洁而准确地描述为递归形式。
递归也是一种基本而典型的算法设计模式。这一模式可以对实际问题中反复出现的结构和形式做高度概括，并从本质层面加以描述与刻画，进而导出高效的算法。从程序结构的角度看，递归模式能够统筹纷繁多变的具体情况，避免复杂的分支以及嵌套的循环，从而更为简明地描述和实现算法，减少代码量，提高算法的可读性，保证算法的整体效率。
### 1.4.1 线性递归
```cpp
int sum(int A[], int n){  // 数组求和算法（线性递归版）
	if (1 > n)  // 平凡情况，递归基
		return 0;  // 直接（非递归式）计算
	else  // 一般情况
		return sum(A, n - 1) + A[n - 1];  // 递归：前n-1项之和，再累计第n-1项
}  // O(1)*递归深度=O(1)*(n+1)=O(n)
```
保证递归算法有穷性的基本技巧：首先判断并处理n=0之类的平凡情况，以免因无限递归而导致系统溢出。这类平凡情况统称 #递归基 （base case of recursion）。平凡情况可能有多种，但至少要有一种，且必然会出现。
1. 算法`sum()`可能朝着更深一层进行自我调用，且每一递归实例对自身的调用至多一次。于是，每一层次上至多只有一个实例，且它们构成一个线性的次序关系。此类递归模式因而称作 #线性递归 （linear recursion），它也是递归的最基本形式。这种形式中，应用问题总可分解为两个独立的子问题：其一对应于单独的某个元素，故可直接求解；另一个对应于剩余部分，且其结构与原问题相同。另外，子问题的解经简单的合并之后，即可得到原问题的解。
2. 线性递归的模式，往往对应于 #减而治之 （decrease-and-conquer）的算法策略：递归每深入一层，待求解问题的规模都缩减一个常数，直至最终蜕化为平凡的小（简单）问题。按照减而治之策略，此处随着递归的深入，调用参数将单调地线性递减。因此无论最初输入的n有多大，递归调用的总次数都是有限的，故算法的执行迟早会终止，即满足有穷性。当抵达递归基时，算法将执行非递归的计算。
### 1.4.2 递归分析
1. 作为一种直观且可视的方法， #递归跟踪 （recursion trace）可用以分析递归算法的总体运行时间和空间。具体地，就是按照以下原则，将递归算法的执行过程整理为图的形式： ^4f111b
	1. 算法的每一递归实例都表示为一个方框，其中注明了该实例调用的参数
	2. 若实例M调用实例N，则在M与N对应的方框之间添加一条有向联线
	3. ![[3_递归跟踪分析.png]]从此图可看出，整个算法所需的计算时间，应该等于所有递归实例的创建、执行和销毁所需的时间总和。其中，递归实例的创建、销毁均由操作系统负责完成，其对应的时间成本通常可以近似为常数，不会超过递归实例中实质计算步骤所需的时间成本，故往往均予忽略。为便于估算，启动各实例的每一条递归调用语句所需的时间，也可以计入被创建的递归实例的账上，如此只需统计各递归实例中非递归调用部分所需的时间。具体地，就`sum()`算法而言，每一递归部分所设计的计算无非三类（判断n是否为0、累加sum(n-1)与A[n-1]、返回当前总和），且至多各执行一次。鉴于它们均属于基本操作，每个递归实例所需的计算时间都应为常数O(3)。对于长度为n的输入数组，递归深度应为n+1，故**整个`sum()`算法的运行时间为：$(n+1)\times O(3)=O(n)$。**`sum()`算法在创建了最后一个递归实例（即到达递归基）时，占用的空间最大，为所有递归实例各自所占空间的总和。这里每一递归实例所需存放的数据，无非是调用参数（数组A的起始地址和长度n）以及用于累加总和的临时变量。这些数据各自只需常数规模的空间，其总量也应为常数。故此可知，`sum()`算法的**空间复杂度线性正比于其递归的深度，即O(n)**。
2. #递推方程 （recurrence equation）法为递归算法的另一常用分析方法。与递归跟踪分析相反，该方法无需绘出具体的调用过程，而是通过对递归模式的数学归纳，导出复杂度定界函数的递推方程（组）及其边界条件，从而将复杂度的分析，转化为递归方程（组）的求解。
	1. 在总体思路上，该方法与微分方程法颇为相似：很多复杂函数的显式表示通常不易直接获得，但是它们的微分形式却往往遵循某些相对简洁的规律，通过求解描述这些规律的一组微分方程，即可最终导出原函数的显式表示。微分方程的解通常并不唯一，除非给定足够多的边界条件。类似的，为使复杂度定界函数的递推方程能够给出确定的解，也需要给定某些边界条件，而这类边界条件往往可以通过对递归基的分析而获得。
	2. 将`sum()`算法中处理长度为n的数组所需的时间成本记作T(n)。该算法的思路为：为解决问题`sum(A,n)`，需要递归地解决问题`sum(A, n-1)`，然后累加上`A[n - 1]`。则求解`sum(A, n)`所需的时间，应该等于求解`sum(A, n - 1)`所需的时间，另加一次整数加法运算所需的时间。则关于T(n)的一般性递推关系为：$T(n)=T(n-1)+O(1)=T(n-1)+c_1$，其中$c_1$是常数；另一方面，当递归过程抵达递归基时，求解平凡问题sum(A, 0)只需（用于直接返回0的）常数时间。如此，即可获得边界条件：$T(0)=O(1)=c_2$，其中$c_2$是常数。联立以上两个方程，最终可以解得：$T(n)=c_1n+c_2=O(n)$。同理也可界定`sum()`算法的空间复杂度。
### 1.4.3 递归模式
1. #多递归基 为保证有穷性，递归算法都必须设置递归基，且确保总能执行到。为此，针对每一类可能出现的平凡情况，都需设置对应的递归基，故同一算法的递归基可能（显式或隐式地）不止一个。
	```cpp
	// 数组倒置问题，即将数组中各元素的次序前后翻转
	// 为得到整个数组的倒置，可以先对换其首、尾元素，然后递归地倒置除这两个元素以外的部分。
	void reverse(int* A, int lo, int hi){  // 数组倒置（多递归基递归版）
		if (lo < hi){
			swap(A[lo], A[hi]);  // 交换A[lo]和A[hi]
			reverse(A, lo+1, hi-1);  // 递归倒置A(lo, hi)
		}  // else隐含了两种递归基
	}  // O(hi - lo + 1)
	```
	```cpp
	void reverse(int*, int, int);  // 重载的倒置算法原型
	void reverse(int* A, int n)  // 数组倒置（算法的初始入口，调用的可能是reverse()的递归版或迭代版）
	{ reverse(A, 0, n-1); }  // 由重载的入口启动递归或迭代算法
	```
2. #多向递归 递归算法中，不仅递归基可能有多个，递归调用也可能有多种可供选择的分支。每一递归实例虽有多个可能的递归方向，但只能从中选择其一，故各层次上的递归实例依然构成一个线性次序关系，这种情况依然属于线性递归。
	1. 就$power(2, n)=2^n$的问题，可按照线性递归的构思，重新定义为$power2(n)=\left\{\begin{matrix} 1 &(若n=0) \\ 2\cdot power2(n-1) &(否则)\end{matrix}\right.$。由此可导出一个线性递归的算法，其复杂度与蛮力的`power2BF_I()`算法完全一样，都为O(n)。
	2. 也能从其它角度分析该函数并给出新的递归定义，使其得以更快地完成幂函数的计算：$power2(n)=\left\{\begin{matrix} 1 &(若n=0) \\ power2(\lfloor n/2\rfloor)^2\times 2 &(若n>0且为奇数) \\ power2(\lfloor n/2\rfloor) &(若n>0且为偶数) \end{matrix}\right.$。按照这一表述，可按二进制展开n之后的各比特位，通过反复的平方运算和加倍运算得到power2(n)。一般地，若n的二进制展开式为$b_1b_2b_3...b_k$，则有$2^n=(...(((1\times 2^{b1})^2\times 2^{b2})^2\times 2^{b3})^2...\times 2^{bk})$；若$n_{k-1}和n_k$的二进制展开式分别为$b_1b_2b_3...b_{k-1}$和$b_1b_2b_3...b_{k-1}b_k$，则有$2^{n_k}=(2^{n_{k-1}})^2\times 2^{bk}$；由此归纳得出递推式：$power2(n_k)=\left\{\begin{matrix} power2(n_{k-1})^2\times 2 &(若b_k=1) \\ power2(n_{k-1})^2 &(若b_k=0)\end{matrix}\right.$。
		```cpp
		inline __int64 sqr(__int64 a){ return a* a };
		__int64 power2(int n){  // 幂函数2^n算法（优化递归版），n>=0
			if (0==n) return 1;  // 递归基；否则，视n的奇偶分别递归
			return (n & 1) ? sqr(power2(n>>1))<<1:sqr(power2(n>>1));
		}  // O(log n)=O(r)，r为输入指数n的比特位数
		```
		针对输入参数n为奇数或偶数的两种可能，这里分别设有不同的递归方向。尽管如此，每个递归实例都只能沿其中的一个方向深入到下层递归，整个算法的递归跟踪分析图的拓补结构仍然与线性递归的类似，故依然属于线性递归。该算法的时间复杂度为：$O(\log n)\times O(1)=O(r)$。与蛮力版本的$O(n)=O(2^r)$相比，计算效率得到了极大提高。
### 1.4.4 递归消除
递归思想可使我们得以从宏观上理解和把握应用问题的实质，深入挖掘和洞悉算法过程的主要矛盾和一般性模式，并最终设计和编写出简洁优美且精确紧凑的算法。然而，递归模式并非十全十美，其众多优点的背后也隐含某些代价。
1. 空间成本：从递归跟踪分析的角度不难看出，递归算法所消耗的空间量主要取决于递归深度，故较之同一算法的迭代版，递归版往往需耗费更多空间，并进而影响实际的运行速度。另外，就操作系统而言，为实现递归调用需要花费大量额外的时间以创建、维护和销毁各递归实例，这些也会令计算的负担雪上加霜。因此，在对运行速度要求极高、存储空间需精打细算的场合，往往应将递归算法改写成等价的非递归版本。一般的转换思路为利用栈结构模拟操作系统的工作过程。
2. 在线性递归算法中，若递归调用在递归实例中恰好以最后一步操作的形式出现，则称作 #尾递归 （tail recursion）。如算法`reverse(A, lo, hi)`的最后一步操作，是对去除了首、尾元素之后总长缩减两个单元的子数组进行递归倒置，即属于典型的尾递归。实际上，属于尾递归形式的算法，均可以简捷地转换为等效的迭代版本。
	```cpp
	void reverse(int* A, int lo, int hi){  // 数组倒置（直接改造而得的迭代版）
	next:  // 算法起始位置添加跳转标志
		if (lo < hi){
			swap(A[lo], A[hi]);  // 交换A[lo]和A[hi]
			lo++;hi--;  // 收缩待倒置区间
			goto next;  // 跳转至算法体的起始位置，迭代地倒置A(lo, hi)
		}  // else隐含了迭代的终止
	}  // O(hi - lo + 1)
	```
	新的迭代版与原递归版功能等效，但其中使用的goto语句有悖于结构化程序设计的原则。因此进一步优化为：
	```cpp
	void reverse(int* A, int lo, int hi){  //数组倒置（规范整理之后的迭代版）
		while (lo < hi)  // 用while替换跳转标志和if，完全等效
			swap(A[lo++], A[hi--]);  // 交换A[lo]和A[hi]，收缩待倒置区间
	}
	```
	尾递归的判断应依据对算法实际执行过程的分析，而不仅仅是算法外在的语法形式。比如，递归语句出现在代码体的最后一行，并不见得就是尾递归；严格地说，只有当该算法（除平凡递归基外）任一实例都终止于这一递归调用时，才属于尾递归。如`sum()`算法实际上就不是尾递归，其最后一次操作是加法运算。
### 1.4.5 二分递归
#递归算法的优化
1. #分而治之策略 （divide-and-conquer）：面对输入规模庞大的应用问题，可将其分解为若干规模更小的子问题，再通过递归机制分别求解。这种分解持续进行，直到子问题规模缩减至平凡情况。
	与减而治之策略一样，这里也要求对原问题重新表述，以保证子问题与原问题在接口形式上的一致。既然每一递归实例都可能做多次递归，故称作 #多路递归 （multi-way recursion）。通常都是将原问题一分为二，故称作 #二分递归 （binary recursion）。无论是分解为两个还是更大常数个子问题，对算法总体的渐进复杂度并无实质影响。
2. 数组求和（二分递归版）
	```cpp
	int sum(int A[], int lo, int hi){  // 数组求和算法（二分递归版，入口为sum(A, 0, n-1)）
		if (lo == hi)  // 如遇递归基（区间长度已降至1）
			return A[lo];  // 则直接返回该元素
		else {  // 否则（一般情况下lo<hi）
			int mi = (lo + hi) >> 1;  // 以居中单元为界，将原区间一分为二
			return sum(A, lo, mi) + sum(A, mi+1, hi);  // 递归对各子数组求和，然后合计
		}
	}  // O(hi-lo+1)，线性正比于区间的长度
	```
	![[4_对sum(A,0,7)的递归跟踪分析.png]]
	上图针对n=8的情况给出了`sum(A, 0, 7)`执行过程的递归跟踪。其中各方框都标注有对应的lo和hi值，即子数组区间的起、止单元。可见，**按照调用的关系及次序，该方法的所有实例构成一个层次结构（即二叉树）**。沿着这个层次结构每下降一层，每个递归实例`sum(lo, hi)`都分裂为一对更小的实例`sum(lo, mi)`和`sum(mi+1, hi)`。每经过一次递归调用，子问题对应的数组区间长度hi-lo+1都将减半。
	算法启动后经连续$m=\log_2n$次递归调用，数组区间的长度从最初的n首次缩减至1，并到达第一个递归基。刚到达任一递归基时，已执行的递归调用总是比递归返回多$m=\log_2n$次。一般地，到达区间长度为$2^k$的任一递归实例之前，已执行的递归调用总是比递归返回多$m-k$次。因此，递归深度（即任一时刻的活跃递归实例的总数）不会超过$m+1$。鉴于每个递归实例仅需常数空间，故除数组本身所占的空间，该算法只需要$O(m+1)=O(\log n)$的附加空间。线性递归版`sum()`算法共需O(n)的附加空间，就这一点而言，新的二分递归版`sum()`算法有很大改进。与线性递归版`sum()`算法一样，此处每一递归实例中的非递归计算都只需要常数时间。递归实例共计2n-1个，故新算法的运行时间为$O(2n-1)=O(n)$，与线性递归版相同。
	此处每个递归实例可向下深入递归两次，故属于多路递归中的二分递归。二分递归与此前介绍的线性递归有很大区别。比如，在线性递归中整个计算过程仅出现一次递归基，而在二分递归过程中递归基的出现相当频繁，总体而言有超过半数的递归实例都是递归基。
3. 除了递归，分治策略算法的计算消耗主要来自两个方面。
	1. **首先是子问题划分，即把原问题分解为形式相同、规模更小的多个子问题**，比如将待求和数组分为前后两段。
	2. **其次是子解答合并，即由递归所得子问题的解，得到原问题的整体解**，比如由子数组之和累加得到整个数组之和。
	3. 为使分治策略真正有效，不仅必须保证这两方面的计算都能高效地实现，还必须保证**子问题之间相互独立：各子问题可独立求解，而无需借助其它子问题的原始数据或中间结果**。否则，或者子问题之间必须传递数据，或者子问题之间需要相互调用，无论如何都会导致时间和空间复杂度的无谓增加。
4. Fibonacci数：二分递归
	计算Fibonacci数列第n项fib(n)，该数列递归形式的定义为：$fib(n)=\left\{\begin{matrix} n &(若n\le 1) \\ fib(n-1) + fib(n-2) &(若n\ge 2) \end{matrix}\right.$。据此定义，可直接导出二分递归版`fib()`算法：
	```cpp
	__int64 fib(int n){  //计算Fibonacci数列的第n项（二分递归版）：O(2^n)
		return (2 > n) ? (__int64) n // 若到达递归基，直接取值
		: fib(n-1) + fib(n-2);  // 否则，递归计算前两项，其和即为正解
	}
	```
	基于Fibonacci数列原始定义的这一实现，虽正确简洁，但需要运行$O(2^n)$时间才能计算出第n项。这一版本`fib()`算法的时间复杂度之所以高达指数量级，究其原因在于，计算过程中所出现的递归实例的重复度极高。
5. 优化策略：为消除递归算法中重复的递归实例，可**借助一定量的辅助空间，在各子问题求解之后，及时记录下其对应的解答。**
	比如，可以从原问题出发自顶而下，每当遇到一个子问题，都首先查验它是否已经计算过，以期通过直接调阅记录获得解答，从而避免重新计算。也可以从递归基出发，自底而上递推得出各子问题的解，直至最终原问题的解。前者即所谓的 #制表策略 （tabulation）或 #记忆策略 （memoization），后者即所谓的 #动态规划策略 （dynamic programming）。
6. Fibonacci数：线性递归
	```cpp
	__int64 fib(int n, __int64& prev){  // 计算Fibonacci数列第n项（线性递归版），接口fib(n, prev)
		if (0 == n){  // 若到达递归基
			prev = 1;
			return 0;
		}  // 则直接取值：fib(-1)=1,fib(0)=0
		else { // 否则
			__int64 prevPrev; prev = fib(n-1, prevPrev);  // 递归计算前两项
			return prevPrev + prev;  // 其和即为正解
		}
	}  // 用辅助变量记录前一项，返回数列的当前项，O(n)
	```
	原二分递归版本中对应于`fib(n-2)`的另一次递归，在这里被省略掉了。其对应的解答，可借助形式参数的机制，通过变量`prevPrev`调阅此前的记录直接获得。该算法呈线性递归模式，递归的升读线性正比于输入n，前后共计出现$O(n)$个递归实例，累计耗时不超过O(n)。但共需使用O(n)规模的附加空间。
7. Fibonacci数：迭代
	线性递归版`fib()`算法中所记录的每一个子问题的解答，只会用到一次。在该算法抵达递归基之后的逐层返回过程中，每向上返回一层，以下各层的解答均不必继续保留。若将以上逐层返回的过程，等效地视作从递归基出发，按规模自小而大求解各子问题的过程，即可采用动态规划策略，将其进一步改写为迭代版：
	```cpp
	__int64 fibI(int n){  // 计算Fibonacci数列的第n项（迭代版）：O(n)
		__int64 f=0,g=1;  // 初始化：fib(0)=0,fib(1)=1
		while (0 < n--){ g += f; f = g - f; }  // 依据原始定义，通过n次加法和减法计算fib(n)
		return f;  // 返回
	}
	```
	这里仅使用了两个中间变量f和g，记录当前的一对相邻Fibonacci数。整个算法仅需线性步数的迭代，时间复杂度为O(n)，空间效率也有了极大提高，仅需常数规模的附加空间。
## 1.5 抽象数据类型
1. 各种数据结构都可看作是由若干数据项组成的集合，同时对数据项定义一组标准的操作。现代数据结构普遍遵从 #信息隐藏 的理念，通过统一接口和内部封装，分层次从整体上加以设计、实现和使用。
2. 所谓 #封装 ，就是将数据项与相关的操作结合为一个整体，并将其从外部的可见性划分为若干级别，从而将数据结构的外部特性与其内部实现相分离，提供一致且标准的对外接口，隐藏内部的实现细节。于是，数据集合及其对应的操作可超脱于具体的程序设计语言、具体的实现方式，即构成所谓的 #抽象数据类型 （abstract data type, ADT）。抽象数据类型的理论催生了现代面向对象的程序设计语言，而支持封装也是此类语言的基本特征。